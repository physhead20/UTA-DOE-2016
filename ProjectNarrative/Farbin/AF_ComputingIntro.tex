Computing costs for HL-LHC are estimated to be at least a factor of 5
more that current LHC~\cite{}, and likely much more due to the
stalling of Moore's Law. While greater use of HPCs in HEP will ideally
alleviate some of the problem, HEP software has yet to harness the
parallelism in the processors, like GPUs, where these systems get most
of their horsepower. Farbin's success in Deep Learning has its roots
in his projects with undergrads on GPUs and the GPU systems they build
and share with XX colleagues and collaborators. [The primary machine
  (2x 12-core Xeon, 128 GB RAM, 50 TB disk) hosts one previous
  generation and three older NVidia GPUs.] Farbin's initial work with
these GPUs was aimed at understanding how to efficiently use them in
the ATLAS software framework resulting in 4 unfunded NSF collaborative
proposals. Section~\ref{} describes how Farbin will direct this thrust
of work towards migration of ATLAS trigger to multi-thread version of
Athena and integration of GPUs. 

Meanwhile, the world is in the midst of a renaissance in Machine
Learning and Artificial Intelligence, known as Deep Learning, driven
by the emergence of large data sets, powerful Graphical Processing
Units (GPUs) processors, and new techniques to train billion-neutron
multi-layer artificial neural networks. Systems trained on raw and
sometimes unlabeled data, can now recognize objects, detect human
emotion and intent, play video games, translate between languages,
generate mathematical proofs, sometimes better than humans and most
importantly with minimal engineering. Developed in University Machine
Learning labs, DL has led Google, Facebook, and other industry-leading
companies to rethink everything, building Artificial Intelligence
teams, software, processors, and cloud services, and demonstrating
impressive feats. DL is the first multi-domain application that
requires and easily harnesses the power of specialized processors,
like GPUs, and supercomputers (i.e. HPCs). Section~\ref{} describes
Farbin's efforts to apply DL to HEP.
